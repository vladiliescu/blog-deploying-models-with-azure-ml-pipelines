{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.core import Pipeline, PipelineRun\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "print(\"AML SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "# Load the workspace from a configuration file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a reference to our auto ml experiment\n",
    "exp = Experiment(ws, 'HousingModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all previous runs in the experiment\n",
    "runs = list(exp.get_runs()) \n",
    "\n",
    "# Get the latest automl run. Alternatively, runs[-1] gets the first run\n",
    "raw_run = runs[0]\n",
    "\n",
    "# Convert the basic `Run` into the richer `AutoMLRun`, to get some extra APIs\n",
    "automl_run = AutoMLRun(exp, raw_run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best output of our automl run..\n",
    "best_run, best_model = automl_run.get_output()\n",
    "\n",
    "# ..and register it in our Models repository\n",
    "automl_run.register_model(model_name='HousePrices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spock, the compute we created when experimenting with automated ml\n",
    "compute = ComputeTarget(workspace=ws, name='Spock')\n",
    "compute.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get a reference to our AmesHousing dataset..\n",
    "ds = Dataset.get_by_name(ws, 'AmesHousing')\n",
    "# ..and convert it to a pipeline input\n",
    "full_ds = ds.as_named_input('full_ds')\n",
    "\n",
    "# Define the step's output\n",
    "fetch_data_param = PipelineData(\"fetched_data\")\n",
    "\n",
    "# Put it all together\n",
    "fetch_step = PythonScriptStep(\n",
    "    name=\"fetch_data\",\n",
    "    script_name=\"fetch.py\",\n",
    "    arguments=[\"--fetched_data\", fetch_data_param],\n",
    "    inputs=[full_ds],\n",
    "    outputs=[fetch_data_param],\n",
    "    compute_target=compute,\n",
    "    source_directory='./fetch_data',\n",
    "    allow_reuse=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to create the directory first\n",
    "!mkdir fetch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fetch_data/fetch.py\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "# Retrieve our input from the current run context\n",
    "ds = Run.get_context().input_datasets['full_ds']\n",
    "df = ds.to_pandas_dataframe()\n",
    "print(df)\n",
    "\n",
    "# Sample 10 houses and make sure to drop the target column\n",
    "forecast_df = df.sample(10).drop(columns='SalePrice')\n",
    "print(forecast_df)\n",
    "\n",
    "# Parse the `fetched_data` argument, this is the location where we should save\n",
    "# the output\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--fetched_data', dest='fetched_data', required=True)\n",
    "args = parser.parse_args()\n",
    "print(args.fetched_data)\n",
    "\n",
    "# Save the output, the AML pipeline infrastructure will take care\n",
    "# of passing it to the next steps\n",
    "forecast_df.to_csv(args.fetched_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the step's output\n",
    "predictions_param = PipelineData(\"predictions\")\n",
    "\n",
    "# Specify manually a configuration\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "# Yes, I would like to be able to specify my dependencies thankyouverymuch\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "# It might be a good idea to pin a specific version of the AML SDK here\n",
    "conda = CondaDependencies()\n",
    "conda.add_pip_package('azureml-sdk[automl]')\n",
    "conda.add_pip_package('joblib')\n",
    "conda.add_pip_package('xgboost==0.90')\n",
    "run_config.environment.python.conda_dependencies = conda\n",
    "\n",
    "# discuss allow reuse for first two steps\n",
    "run_step = PythonScriptStep(\n",
    "    name=\"run\",\n",
    "    script_name=\"run.py\",\n",
    "    arguments=[\"--fetched_data\", fetch_data_param, \"--predictions\", predictions_param],\n",
    "    inputs=[fetch_data_param],\n",
    "    outputs=[predictions_param],\n",
    "    compute_target=compute,\n",
    "    runconfig = run_config,\n",
    "    source_directory='./run',\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run/run.py\n",
    "\n",
    "from azureml.core import Run, Model, Workspace\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Parse arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--fetched_data', dest='fetched_data', required=True)\n",
    "parser.add_argument('--predictions', dest='predictions', required=True)\n",
    "args = parser.parse_args()\n",
    "print(args.fetched_data)\n",
    "print(args.predictions)\n",
    "\n",
    "# Read the input data\n",
    "df = pd.read_csv(args.fetched_data)\n",
    "print(df)\n",
    "\n",
    "# Get the current context's workspace..\n",
    "ws = Run.get_context().experiment.workspace\n",
    "print(ws)\n",
    "\n",
    "# ..in order to be able to retrieve a model from the repository..\n",
    "model_ws = Model(ws, 'HousePrices')\n",
    "\n",
    "# ..which we'll then download locally..\n",
    "pickled_model_name = model_ws.download(exist_ok = True)\n",
    "\n",
    "# ..and deserialize\n",
    "model = joblib.load(pickled_model_name)\n",
    "print(model)\n",
    "\n",
    "# ..and use to predict the house prices\n",
    "results = model.predict(df)\n",
    "print(results)\n",
    "\n",
    "# The predictions are stored in the `predictions` output path\n",
    "# so that AML can find them and pass them to other steps\n",
    "df['PredictedSalePrice'] = results\n",
    "df.to_csv(args.predictions, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_step = PythonScriptStep(\n",
    "    name=\"save_predictions\",\n",
    "    script_name=\"save.py\",\n",
    "    arguments=[\"--predictions\", predictions_param],\n",
    "    inputs=[predictions_param],\n",
    "    compute_target=compute,\n",
    "    source_directory='./save_predictions',\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir save_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile save_predictions/save.py\n",
    "\n",
    "from azureml.core import Run, Model, Workspace\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Parse arguments and print the `predictions` input\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--predictions', dest='predictions', required=True)\n",
    "args = parser.parse_args()\n",
    "print(args.predictions)\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(args.predictions)\n",
    "print(df)\n",
    "\n",
    "# Get a reference to the workspace's default data store, we'll use this\n",
    "# to save the predictions\n",
    "ws = Run.get_context().experiment.workspace\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Create a folder and persist the predictions inside\n",
    "os.mkdir('./out')\n",
    "df.to_csv('./out/predictions.csv')\n",
    "\n",
    "# Upload the folder to the workspace's default data store\n",
    "ds.upload('./out', target_path='latest_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[fetch_step, run_step, save_step])\n",
    "pipeline.validate()\n",
    "pipeline.submit('IRunPipelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the pipeline first, so that we can reference it when defining the schedule\n",
    "published_pipeline = pipeline.publish()\n",
    "\n",
    "# Run twice a day, every day\n",
    "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=1, hours=[1, 13], minutes=[30])\n",
    "recurring_schedule = Schedule.create(ws, name=\"DailySchedule\", \n",
    "                            description=\"Twice a day, at 01:30 and 13:30\",\n",
    "                            pipeline_id=published_pipeline.id, \n",
    "                            experiment_name='IRunScheduledPipelines', \n",
    "                            recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = Schedule.list(ws, pipeline_id=published_pipeline.id)\n",
    "schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable/enable all schedules of a pipeline\n",
    "for schedule in schedules:\n",
    "    schedule.disable()\n",
    "    #schedule.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
